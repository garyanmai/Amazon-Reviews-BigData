{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "14c28878-4ad1-4b08-9fc6-021e7095f68d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Amazon Reviews Feature Engineering and Modeling Script\n",
    "\n",
    "For this project, I will only be using the 'amazon_reviews_us_Beauty_v1_00.tsv' data set. This file was downloaded individually on the AWS CLI using an API command provided by the link below. All files in this project will be stored in it's respective folders from my AWS S3 bucket. All scripts were produced using the Databricks Community Edition.\n",
    "\n",
    "- **Link to Kaggle Data Set**: https://www.kaggle.com/datasets/cynthiarempel/amazon-us-customer-reviews-dataset\n",
    "- **Link to Databricks Community Edition**: https://docs.databricks.com/en/getting-started/community-edition.html\n",
    "\n",
    "**Note: This dataset is 22 GB. Please consider copying the API command that is available at the link to download individual files that you will be working with.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3ddef095-6f07-45f8-ae9e-2b4363357d8c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Import `os` and set up the environment variables for your AWS Access Keys\n",
    "**Disclaimer: It is important that you do not share your AWS access/secret keys with anyone. Please double-check your script before posting/sharing your script to the public**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b20032b-42bf-4cc4-914b-099fdeaefdff",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# To work with Amazon S3 storage, set the following variables using your AWS Access Key and Secret Key\n",
    "# Set the Region to where your files are stored in S3.\n",
    "\n",
    "# Replace 'your-access-key' and 'your-secret-key' with your actual AWS access/secret keys\n",
    "access_key = 'your-access-key'\n",
    "secret_key = 'your-secret-key'\n",
    "\n",
    "# Set the environment variables so boto3 can pick them up later\n",
    "os.environ['AWS_ACCESS_KEY_ID'] = access_key\n",
    "os.environ['AWS_SECRET_ACCESS_KEY'] = secret_key\n",
    "encoded_secret_key = secret_key.replace(\"/\", \"%2F\")\n",
    "\n",
    "# Note: You may need to change the aws_region depending on where your AWS S3 is located\n",
    "aws_region = \"us-east-2\"\n",
    "\n",
    "# Update the Spark options to work with our AWS Credentials\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", access_key)\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", secret_key)\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", \"s3.\" + aws_region +\n",
    "\".amazonaws.com\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6985c866-2d30-4ddd-aed0-cd8c15f838ad",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Install all necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85b44371-7859-4861-9b65-839af53f2def",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Pin the s3fs and fsspec versions\n",
    "%pip install \"s3fs==2023.12.1\" \"fsspec==2023.12.1\"\n",
    "%pip install textblob\n",
    "%pip install boto3\n",
    "%pip install --upgrade botocore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "efce817e-11ee-43d2-95e6-1ba3f132b260",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Import all necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6eb37f05-0613-43b8-9e57-cf64af57437a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import pandas as pd\n",
    "import s3fs\n",
    "import boto3\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "from pyspark.sql.functions import col, isnan, when, count, udf\n",
    "from IPython.display import HTML, display\n",
    "from datetime import datetime\n",
    "from pyspark.sql.functions import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5c1a7a32-6e07-4cf9-a2be-f856815679b2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Create File Path for your AWS S3 Bucket File(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98022378-c3d4-49ed-92ea-05b1e060d25e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Set up the path to an Amazon reviews data stored on S3\n",
    "# replace 'bucket-name-xx' with your actual bucket name. Replace 'xx' with your initials.\n",
    "bucket = 'bucket-name-xx/raw/'\n",
    "filename = 'amazon_reviews_us_Beauty_v1_00_cleaned.parquet'\n",
    "file_path = 's3a://' + bucket + filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4bf7e88b-7846-4644-bc1b-fe535b53429b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Create a Spark Dataframe from the Parquet file from your AWS S3 Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "169c3cd3-c2b1-422e-a27f-5ce85d15670b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Since this is the cleaned filed and saved as a parquet, it should only take about 2 seconds to load\n",
    "sdf = spark.read.parquet(file_path, inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96c508ac-38f4-4c62-af56-6140eb614bf6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Take a small sample for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46396b04-2614-4e07-8c5e-5af5f49b096d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Take a small sample (for now) when testing, you can remove this to test the whole dataset after\n",
    "sdf = sdf.sample(False, 0.01)\n",
    "sdf.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "22caa3ba-7b6a-490a-a1ce-8f5e6514027f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Check the Data Types of your Spark DataFrame\n",
    "- **This step is crucial for understanding how you will want to proceed to feature engineering.**\n",
    "- **Here is how you should consider mapping out your data type conversion in preparation for feature engineering:**\n",
    "- **String** --> String Indexer --> Encoder --> Vector Assembler\n",
    "- **Long/Int** --> Encoder --> Vector Assembler\n",
    "- **Float/Double** --> Vector Assembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "597f1b47-eb3e-4124-b7c2-34312843712d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check the schema\n",
    "sdf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "615ba6f2-7d62-42aa-9a74-5d6ad21fce9a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Import all necessary modules for Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b04ad2df-6281-45c3-a446-da4dadb0dc32",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Importing functions and modules\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.feature import *\n",
    "# Import the evaluation module\n",
    "from pyspark.ml.evaluation import *\n",
    "# Import the model tuning module\n",
    "from pyspark.ml.tuning import *\n",
    "from pyspark.ml import Pipeline\n",
    "# Import the logistic regression model\n",
    "from pyspark.ml.classification import LogisticRegression, LogisticRegressionModel\n",
    "import numpy as np\n",
    "from pyspark.sql.types import DoubleType\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "210d00c1-ea94-4c55-8467-4cab305e93a9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# 1. Create Indexer, Encoder, and MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ccd5c9d-3e25-4d57-91f8-47087d601c0d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# This wil take 0.5 seconds to run on a 0.01 sample size! (compared to around 8-9 minutes on the full dataset)\n",
    "# Adding Indexers, Encoders  and Assembler\n",
    "\n",
    "# Note: since we are predicting star_rating, we cannot have it as a feature, thus it will not need to be binarized\n",
    "sdf = sdf.withColumn(\"label\", when(sdf.star_rating > 3, 1.0).otherwise(0.0).cast(DoubleType()))\n",
    "\n",
    "# Create an indexer for the string based columns\n",
    "# We can omit product_title in this step\n",
    "indexer_input_columns =  [\"product_category\", \"review_yearmonth\", \"review_dayofweek\", \"vine\", \"verified_purchase\"]\n",
    "indexer_output_columns = [\"product_category_Index\", \"review_yearmonth_Index\", \"review_dayofweek_Index\", \"vine_Index\", \"verified_purchase_Index\"]\n",
    "indexer = StringIndexer(inputCols=indexer_input_columns, outputCols=indexer_output_columns)\n",
    "\n",
    "# Create an encoder \n",
    "encoder_input_columns = [\"product_category_Index\", \"review_yearmonth_Index\", \"review_dayofweek_Index\", \"vine_Index\", \"verified_purchase_Index\"]\n",
    "encoder_output_columns = [\"product_category_Vector\", \"review_yearmonth_Vector\", \"review_dayofweek_Vector\", \"vine_Vector\", \"verified_purchase_Vector\"]\n",
    "encoder = OneHotEncoder(inputCols=encoder_input_columns, outputCols=encoder_output_columns, dropLast=False)\n",
    "\n",
    "# Explicitly cast the two integer columns: \"review_year\" and \"review_month\" to DoubleType to ensure consistency of data types\n",
    "sdf = sdf.withColumn(\"review_year\", col(\"review_year\").cast(DoubleType()))\n",
    "sdf = sdf.withColumn(\"review_month\", col(\"review_month\").cast(DoubleType()))\n",
    "\n",
    "# Scale the helpful_votes and total_votes column\n",
    "helpful_votes_assembler = VectorAssembler(inputCols=['helpful_votes'], outputCol='helpful_votes_Vector')\n",
    "total_votes_assembler = VectorAssembler(inputCols=['total_votes'], outputCol='total_votes_Vector')\n",
    "# Scaler\n",
    "helpful_votes_scaler = MinMaxScaler(inputCol=\"helpful_votes_Vector\", outputCol=\"helpful_votes_Scaled\")\n",
    "total_votes_scaler = MinMaxScaler(inputCol=\"total_votes_Vector\", outputCol=\"total_votes_Scaled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "063c13af-f0ee-470f-8798-de748aa39a63",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# 2. Create Tokenizer, HasherTF, and IDF for text-based columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a55ee48-da80-4251-939e-8e3e7b12ac1d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Note: We can use the same 'sdf' DataFrame each time\n",
    "# Tokenizer\n",
    "sdf = sdf.drop(\"review_words\")\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"review_body\", outputCol=\"review_words\", pattern=\"\\\\w+\", gaps=False)\n",
    "sdf = regexTokenizer.transform(sdf)\n",
    "sdf.select(\"review_body\", \"review_words\").show(truncate=False)\n",
    "\n",
    "# HashingTF\n",
    "sdf = sdf.drop(\"review_word_features\")\n",
    "hashingTF = HashingTF(numFeatures=4096, inputCol=\"review_words\", outputCol=\"review_word_features\")\n",
    "sdf = hashingTF.transform(sdf)\n",
    "sdf.select(['review_words','review_word_features']).show(truncate=False)\n",
    "\n",
    "# Inverse Document Frequency (IDF)\n",
    "idf = IDF(inputCol='review_word_features', outputCol=\"review_body_features\", minDocFreq=1)\n",
    "sdf = idf.fit(sdf).transform(sdf)\n",
    "sdf.select(\"review_date\", \"review_body_features\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ccf2b4a9-4fdb-492f-88ad-d8dfdd97a112",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Tokenizer\n",
    "#sdf = sdf.drop(\"review_words\")\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"review_headline\", outputCol=\"review_headline_words\", pattern=\"\\\\w+\", gaps=False)\n",
    "sdf = regexTokenizer.transform(sdf)\n",
    "sdf.select(\"review_headline\", \"review_headline_words\").show(truncate=False)\n",
    "\n",
    "# HashingTF\n",
    "#sdf = sdf.drop(\"review_word_features\")\n",
    "hashingTF = HashingTF(numFeatures=4096, inputCol=\"review_headline_words\", outputCol=\"review_headline_word_features\")\n",
    "sdf = hashingTF.transform(sdf)\n",
    "sdf.select(['review_headline_words','review_headline_word_features']).show(truncate=False)\n",
    "\n",
    "# Inverse Document Frequency (IDF)\n",
    "idf = IDF(inputCol='review_headline_word_features', outputCol=\"review_headline_features\", minDocFreq=1)\n",
    "sdf = idf.fit(sdf).transform(sdf)\n",
    "sdf.select(\"review_date\", \"review_headline_features\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c87e466e-5b20-436e-a599-5e356ba2d3b4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# 3. Create an assembler for the individual feature vectors and the float/double columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1424b73-909e-4222-b4a5-c8f7ffa8c47b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create an assembler\n",
    "assembler = VectorAssembler(inputCols=[\"product_category_Vector\", \"review_yearmonth_Vector\", \"review_dayofweek_Vector\",  \"helpful_votes_Scaled\", \"total_votes_Scaled\", \"vine_Vector\", \"verified_purchase_Vector\", \"review_body_features\", \"review_headline_features\", \"review_weekend\"], \n",
    "                            outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ffaab77d-8190-47f7-a99d-8bce1e261113",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# 4. Create the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c7a59e0-133f-40cb-9483-eb8f429523d7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create the Pipeline\n",
    "# Note: since we already ran the  regexTokenizer, hashingTF, idf, those are now part of the 'sdf' dataframe, it does not need to be in the assembler\n",
    "reviews_pipe = Pipeline(stages=[indexer, encoder, helpful_votes_assembler, total_votes_assembler, helpful_votes_scaler, total_votes_scaler,  assembler])\n",
    "\n",
    "# Call .fit to transform the data\n",
    "transformed_sdf = reviews_pipe.fit(sdf).transform(sdf)\n",
    "\n",
    "# Review the transformed features\n",
    "print(\"Transformed features\")\n",
    "\n",
    "transformed_sdf.select('features','label').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b68a8a22-8565-4339-96e1-ae652bfc501a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# 5. Plug new sdf into the Logistic Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6baa9283-986e-469d-8060-5624fd90a636",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# At this point now, we have a decent looking features and label\n",
    "# Now, we can plug that in to the Logistic Regression model.\n",
    "# This will take about 9-11 minutes to run\n",
    "trainingData, testData = transformed_sdf.randomSplit([0.7, 0.3], seed=42)\n",
    "\n",
    "# Create a LogisticRegression Estimator\n",
    "lr = LogisticRegression()\n",
    "\n",
    "# Fit the model to the training data\n",
    "model = lr.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce9e2d0e-61c2-4e5c-9c37-f393bda5b327",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Save model to S3 bucket in the /model directory\n",
    "# Replace 'bucket-name-xx' with your actual bucket name and replace 'xx' with your initials\n",
    "# For the file name, replace 'yyyy-mm-dd' with the today's date\n",
    "model_path = \"s3://bucket-name-xx/models/logistic_regression_model_yyyy-mm-dd\"\n",
    "model.write().overwrite().save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4247b561-07fe-488c-8126-3807528a5002",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# This will take about 2 minutes to run\n",
    "# Show model coefficients and intercept\n",
    "print(\"Coefficients: \", model.coefficients)\n",
    "print(\"Intercept: \", model.intercept)\n",
    "\n",
    "# Test the model on the testData\n",
    "test_results = model.transform(testData)\n",
    "\n",
    "# Show the test results\n",
    "test_results.select('product_title','product_category', 'helpful_votes', 'total_votes', 'verified_purchase', 'review_body', 'review_date', 'probability', 'prediction').show()\n",
    "\n",
    "# Show the confusion matrix\n",
    "test_results.groupby('label').pivot('prediction').count().sort('label').show()\n",
    "\n",
    "confusion_matrix = test_results.groupby('label').pivot('prediction').count().fillna(0).collect()\n",
    "\n",
    "def calculate_recall_precision(confusion_matrix):\n",
    "    tn = confusion_matrix[0][1]  # True Negative\n",
    "    fp = confusion_matrix[0][2]  # False Positive\n",
    "    fn = confusion_matrix[1][1]  # False Negative\n",
    "    tp = confusion_matrix[1][2]  # True Positive\n",
    "    precision = tp / ( tp + fp )            \n",
    "    recall = tp / ( tp + fn )\n",
    "    accuracy = ( tp + tn ) / ( tp + tn + fp + fn )\n",
    "    f1_score = 2 * ( ( precision * recall ) / ( precision + recall ) )\n",
    "    return accuracy, precision, recall, f1_score\n",
    "\n",
    "print(\"Accuracy, Precision, Recall, F1 Score\")\n",
    "print( calculate_recall_precision(confusion_matrix) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "56f9dcb0-bd6a-455a-ae43-9792fbb065c1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# 6. Data Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2b8dbebe-7f1d-456e-a981-a7188799679d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**1. Linear Regression Model based on helpful_votes_Scaled and total_votes_Scaled** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dac6b4dc-2e84-4152-a5fa-89cfb99c0d8d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Linear Regression Model based on helpful_votes_Scaled and total_votes_Scaled\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Extract features and labels\n",
    "X = np.array(transformed_sdf.select('total_votes_Scaled').collect()).reshape(-1, 1)\n",
    "y = np.array(transformed_sdf.select('helpful_votes_Scaled').collect()).ravel()\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Create a linear regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict the target variable\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Plot the regression line\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_test, y_test, color='red', marker='o', label='Actual Data')\n",
    "plt.plot(X_test, y_pred, label='Linear Regression Line', color='blue')\n",
    "plt.xlabel('Total Votes')\n",
    "plt.ylabel('Helpful Votes (Scaled)')\n",
    "plt.title('Linear Regression')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f83300f-4224-44b5-b582-227b19caa4fc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "# Calculate RMSE\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(f'Root Mean Squared Error (RMSE): {rmse}')\n",
    "\n",
    "# Calculate R-squared\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f'R-squared (R2): {r2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3288df36-5d86-41d2-8927-6cb3f95fb8b8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**2. Logistic Regression Model based on total_votes_Scaled and label** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f4b8921-6401-4513-b6f9-b49b8e639ef6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Logistic Regression Model based on total_votes_Scaled and star_rating ('label'= star_rating > 3 = 1.0) \n",
    "# Note: a Star Rating of 1.0 is a star rating that is more than 3, which indicates a \"good\" rating\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Extract features and labels\n",
    "X = np.array(transformed_sdf.select('total_votes_Scaled').collect()).reshape(-1, 1)\n",
    "y = np.array(transformed_sdf.select('label').collect()).ravel()\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Create a logistic regression model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Plot the logistic regression curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Generate a range of values for the star_rating\n",
    "rating_range = np.linspace(X[:, 0].min(), X[:, 0].max(), 300).reshape(-1, 1)\n",
    "\n",
    "# Calculate the predicted probabilities\n",
    "probabilities = model.predict_proba(rating_range)[:, 1]\n",
    "\n",
    "# Plot the logistic regression curve\n",
    "plt.plot(rating_range, probabilities, label='Logistic Regression Curve', color='blue')\n",
    "\n",
    "# Scatter plot of the data points\n",
    "plt.scatter(X[:, 0], y, color='red', marker='o', label='Actual Data')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Star Rating')\n",
    "plt.ylabel('Probability of Total Votes')\n",
    "plt.title('Logistic Regression Curve')\n",
    "\n",
    "# Show the legend\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ac4c1bf2-2c55-4440-b92a-33505f5a7c87",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**3. Logistic Regression Model based on review_body_length and 'label'**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5bd7276-46c6-4640-88a4-fc6f1818a0f4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Logistic Regression Model based on review_body_length and 'label'\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Extract features and labels\n",
    "X = np.array(transformed_sdf.select('review_body_length').collect())\n",
    "y = np.array(transformed_sdf.select('label').collect()).ravel()\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Create a logistic regression model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Plot the logistic regression curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Generate a range of values for the review_body_length\n",
    "length_range = np.linspace(X[:, 0].min(), X[:, 0].max(), 300).reshape(-1, 1)\n",
    "\n",
    "# Calculate the predicted probabilities\n",
    "probabilities = model.predict_proba(length_range)[:, 1]\n",
    "\n",
    "# Plot the logistic regression curve\n",
    "plt.plot(length_range, probabilities, label='Logistic Regression Curve', color='blue')\n",
    "\n",
    "# Scatter plot of the data points\n",
    "plt.scatter(X[:, 0], y, color='red', marker='o', label='Actual Data')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Review Body Length')\n",
    "plt.ylabel('Probability of Label 1')\n",
    "plt.title('Logistic Regression Curve')\n",
    "\n",
    "# Show the legend\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a978d16a-8c22-4d39-9758-0e742377e0c6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Creating a Logistic Regression Model using Sentiment Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6950716c-7726-47e6-bd8a-66060cf0e63a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Creating a Logistic Regression Model using sentiment analysis \n",
    "# Defining sentiment analysis function\n",
    "from textblob import TextBlob\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "# Create a function to perform sentiment analysis on some text\n",
    "def sentiment_analysis(some_text):\n",
    "    sentiment = TextBlob(some_text).sentiment.polarity\n",
    "    return sentiment\n",
    "\n",
    "sentiment_analysis_udf = udf(sentiment_analysis, DoubleType())\n",
    "\n",
    "# Apply sentiment analysis UDF\n",
    "transformed_sdf = sdf.withColumn(\"sentiment_score_review\", sentiment_analysis_udf(sdf['review_body']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bc5be605-af90-467b-99e9-78d6c71149d8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**4. Logistic Regression Model based on 'sentiment_score_review' and 'label'**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "396f06e6-5916-474f-9629-d39d7ae19c32",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Logistic Regression Model based on 'sentiment_score_review' and 'label'\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Extract features and labels\n",
    "X = np.array(transformed_sdf.select('sentiment_score_review').collect())\n",
    "y = np.array(transformed_sdf.select('label').collect()).ravel()\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Create a logistic regression model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Plot the logistic regression curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Generate a range of values for the sentiment_score_review\n",
    "score_range = np.linspace(X[:, 0].min(), X[:, 0].max(), 300).reshape(-1, 1)\n",
    "\n",
    "# Calculate the predicted probabilities\n",
    "probabilities = model.predict_proba(score_range)[:, 1]\n",
    "\n",
    "# Plot the logistic regression curve\n",
    "plt.plot(score_range, probabilities, label='Logistic Regression Curve', color='blue')\n",
    "\n",
    "# Scatter plot of the data points\n",
    "plt.scatter(X[:, 0], y, color='red', marker='o', label='Actual Data')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Sentiment Score Review')\n",
    "plt.ylabel('Probability of Label 1')\n",
    "plt.title('Logistic Regression Curve')\n",
    "\n",
    "# Show the legend\n",
    "plt.legend()\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig(\"label_by_sentiment_score_matplotlib.png\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6f27ae89-b4d0-430b-8bca-04cc7557dfe1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Save Figure to your AWS S3 Bucket Folder '/models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a5249a5-d586-445c-aef0-5e3b70d82647",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Save Figure to your AWS S3 Bucket Folder '/models'\n",
    "from io import BytesIO\n",
    "\n",
    "# Save the figure to a BytesIO object\n",
    "img_data = BytesIO()\n",
    "plt.savefig(img_data, format='png')\n",
    "img_data.seek(0)\n",
    "\n",
    "# Your S3 bucket details\n",
    "# Replace 'bucket-name-xx' with your actual bucket name. Replace 'xx' with your initials.\n",
    "bucket_name = 'bucket-name-xx'\n",
    "s3_path = 'models/'\n",
    "\n",
    "# Upload the figure to S3 using boto3\n",
    "# Replace 'your-access-key' and 'your-secret-key' with your actual AWS access/secret keys.\n",
    "s3 = boto3.client('s3', aws_access_key_id='your-access-key', aws_secret_access_key='your-secret-key')\n",
    "s3.upload_fileobj(img_data, bucket_name, s3_path)\n",
    "\n",
    "# Close the plot\n",
    "plt.close()\n",
    "\n",
    "# Print statement\n",
    "print(f\"Figure saved to S3: {bucket_name}/{s3_path}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Final_FeatureEngineering_Modeling_Script",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
