{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c4179a3c-1883-420c-b23f-9f074b83ea00",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Amazon Reviews Exploratory Data Analysis and Cleaning Script\n",
    "\n",
    "For this project, I will only be using the 'amazon_reviews_us_Beauty_v1_00.tsv' data set. This file was downloaded individually on the AWS CLI using an API command provided by the link below. All files in this project will be stored in it's respective folders from my AWS S3 bucket. All scripts were produced using the Databricks Community Edition.\n",
    "\n",
    "- **Link to Kaggle Data Set**: https://www.kaggle.com/datasets/cynthiarempel/amazon-us-customer-reviews-dataset\n",
    "- **Link to Databricks Community Edition**: https://docs.databricks.com/en/getting-started/community-edition.html\n",
    "\n",
    "**Note: This dataset is 22 GB. Please consider copying the API command that is available at the link to download individual files that you will be working with.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "405874e4-7f33-43d8-99a5-e26ee6a98473",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Import `os` and set up the environment variables for your AWS access/secret Keys\n",
    "**Disclaimer: It is important that you do not share your AWS access/secret keys with anyone. Please double-check your script before posting/sharing your script to the public**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "672664e4-04b4-4d2f-bb92-211e4fd4ad0e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# To work with Amazon S3 storage, set the following variables using your AWS Access Key and Secret Key\n",
    "# Set the Region to where your files are stored in S3.\n",
    "# Replace 'your-access-key' and 'your-secret-key' with your actual AWS access/secret keys\n",
    "access_key = 'your-access-key'\n",
    "secret_key = 'your-secret-key'\n",
    "# Set the environment variables so boto3 can pick them up later\n",
    "os.environ['AWS_ACCESS_KEY_ID'] = access_key\n",
    "os.environ['AWS_SECRET_ACCESS_KEY'] = secret_key\n",
    "encoded_secret_key = secret_key.replace(\"/\", \"%2F\")\n",
    "# Note: You may need to change the aws_region depending on where your AWS S3 is located\n",
    "aws_region = \"us-east-2\"\n",
    "# Update the Spark options to work with our AWS Credentials\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", access_key)\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", secret_key)\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", \"s3.\" + aws_region +\n",
    "\".amazonaws.com\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "89d549bd-8fba-4121-a08e-ce8d83ea19c4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Install all necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8c306d6-d2da-4924-8d9e-b53bd3241160",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    " %pip install fsspec\n",
    " %pip install s3fs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c7fed78f-3166-4471-8097-aa1b70ae0e42",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Import all necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5d7a35e-effd-497e-b059-709ddbd73503",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import pandas as pd\n",
    "import s3fs\n",
    "import boto3\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "from IPython.display import HTML, display\n",
    "from datetime import datetime\n",
    "from pyspark.sql.functions import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cdd7546f-6411-43ec-aebf-b66258c78d7a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Create File Path for your AWS S3 Bucket File(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e495f4d0-e525-4696-bde9-0d4ef34c082a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Set up the path to an Amazon reviews data stored on S3\n",
    "# replace 'bucket-name-xx' with your actual bucket name. Replace 'xx' with your initials.\n",
    "bucket = 'bucket-name-xx/landing/'\n",
    "filename = 'amazon_reviews_us_Beauty_v1_00.tsv'\n",
    "file_path = 's3a://' + bucket + filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b51e29bb-bf33-4d8b-ab32-d31ca68bb234",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Create a Spark Dataframe from the file from your AWS S3 Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7bb282a-db3f-4ec3-bcb8-c8150029656b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# This can take a long time if the file is big\n",
    "# This one in particular takes about 1.10 minutes\n",
    "sdf = spark.read.csv(file_path, sep='\\t', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9a70f30e-603c-4c5b-83c4-e504797d8586",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Check the Data Types of your Spark DataFrame\n",
    "- **This step is crucial for understanding how you will want to proceed to feature engineering.**\n",
    "- **Here is how you should consider mapping out your data type conversion in preparation for feature engineering:**\n",
    "- **String** --> String Indexer --> Encoder --> Vector Assembler\n",
    "- **Long/Int** --> Encoder --> Vector Assembler\n",
    "- **Float/Double** --> Vector Assembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a55e0df-5960-46d3-8e91-f3abc0a9ba40",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check the schema\n",
    "sdf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "77c0ba76-a47d-4540-bf1d-b5482118a473",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Count the Null/NaN values in your Spark DataFrame\n",
    "- **This helps us understand how many records or columns will need to be cleaned/dropped/or back-filled (if-neccessary)**\n",
    "- **We will also need to do this before we start converting our data types in order to avoid any errors in conversion.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "584f43fa-fa2c-4497-bc0b-6e304721038c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check to see the NULL/NaN values in your first 5 columns \n",
    "# Here, we are checking 5 columns at a time so that the visual output doesn't get too cluttered\n",
    "sdf.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in\n",
    "[\"marketplace\", \"customer_id\", \"review_id\", \"product_id\", \"product_parent\"]] ).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60df5eb5-5941-4d62-b10b-26d898e54666",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check to see NULL/NaN values for the next 5 columns\n",
    "# Note: You should see that product_category, star_rating, helpful_votes, and total_votes have 213-214 null values\n",
    "sdf.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in\n",
    "[\"product_title\", \"product_category\", \"star_rating\", \"helpful_votes\", \"total_votes\"]] ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2a4edf3-4fe5-49a0-a6ff-358d8ad955bd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check to see NULL/NaN values for the remaining 4 columns\n",
    "# Note: We will check for Null values from review_date in another cell, since isnan cannot be executed with \"date\" data types\n",
    "# Running isnan on a date data type will result in a data mismatch error\n",
    "# You should see that vine, verified purchase, and review_headlines has 214-223 null/nan values\n",
    "# You should also see that review_body has 916 null/nan values\n",
    "sdf.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in\n",
    "[\"vine\", \"verified_purchase\", \"review_headline\", \"review_body\"]] ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c56f33e6-0de7-4f44-8db0-f83a0d44602b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Count only the null values for the review_date column and show the count\n",
    "# You should see that review_date has 563 null values\n",
    "null_count = sdf.agg(count(when(col(\"review_date\").isNull(), \"review_date\"))).collect()[0][0]\n",
    "\n",
    "print(f\"Review Date Column Null Count: {null_count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f264579-906f-4f34-aafb-ac2eba6046fc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Get the Number of Records in the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce1f68e1-4524-4272-84a1-3b4ef5af2bd7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# We will compare this number to the count after cleaning for Null/NaN values to see how much was cleaned\n",
    "# You should see that this DataFrame has 5,115,666 million records\n",
    "sdf.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "da8dd721-23ff-4200-9577-c0155607d758",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Drop the Null/NaN records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30a7131b-ffca-4466-9bb9-0b3ab929d3a0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Drop some of the records where the certain columns are empty (Null/NaN)\n",
    "sdf = sdf.na.drop(subset=[\"product_category\", \"star_rating\", \"helpful_votes\", \"total_votes\", \"vine\", \"verified_purchase\", \"review_headline\", \"review_body\", \"review_date\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "919ad033-c45f-42fc-b904-f2f23fb3b17f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Double-check to see if the columns still have Null/NaN values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d09ac52-dac4-41ca-8a2f-d0c9a0f33f0f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# You should see that the review_headline still have a Null/NaN value\n",
    "sdf.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in\n",
    "[\"product_category\", \"star_rating\", \"helpful_votes\", \"total_votes\", \"vine\", \"verified_purchase\", \"review_headline\", \"review_body\"]] ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3fc766a-ec63-4e20-b3d8-d2c2079127a5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Re-count only the null values for the review_date column and show the count\n",
    "# You should see that review_date has 0 null values\n",
    "null_count = sdf.agg(count(when(col(\"review_date\").isNull(), \"review_date\"))).collect()[0][0]\n",
    "\n",
    "print(f\"Review Date Column Null Count: {null_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "150bd7bc-7187-48f9-af10-1c6404497bfb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Drop any un-necessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "155988e0-f0ac-4a6d-8a73-5bb09672cd56",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Drop the un-necessary columns. Since the review_date is our most unique attribute, we can keep it in our Spark DataFrame.\n",
    "# Later, we will see how we can split the review_date column into more aggregable columns!\n",
    "sdf = sdf.drop(\"marketplace\", \"customer_id\", \"review_id\", \"product_id\", \"product_parent\")\n",
    "\n",
    "# Display schema\n",
    "sdf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e71d9226-0aa3-45fa-87bf-c60b91a9c5c2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Get rid of any non-ascii characters\n",
    "**By removing non-ASCII characthers, we can standardize the text. Non-ASCII characters can contain special symbols, accented letters, and characters from various languages that may weaken our predictive modeling power later on.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42ff4e14-ed6c-4b67-9294-cfb433675c65",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define a function to strip out any non-ascii characters\n",
    "def ascii_only(mystring):\n",
    "    if mystring:\n",
    "        return mystring.encode('ascii', 'ignore').decode('ascii')\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b02af2e4-a65c-475d-9fc8-dc0bdc27f3a7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Turn this function into a User-Defined Function (UDF)\n",
    "ascii_udf = udf(ascii_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09b0b441-9cb4-4b26-ad02-b233a033d6d7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Clean up the review_body\n",
    "sdf = sdf.withColumn(\"review_body\", ascii_udf('review_body'))\n",
    "\n",
    "# Display schema\n",
    "sdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4eb4e54-5dee-4f92-afbe-8a6cad0f3bbf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check the length of the text in review_body and review_headline\n",
    "sdf = sdf.withColumn(\"review_body_length\", length(sdf.review_body))\n",
    "sdf = sdf.withColumn(\"review_headline_length\", length(sdf.review_headline))\n",
    "\n",
    "sdf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7479cca6-9828-43f7-9acf-1e7847b30f20",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Check the statistics of the review body and review headline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b06df900-7c0f-4b1f-b2f3-2cea8378f520",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Some may have a minimum of 1, which may be an emoji that can't be encoded\n",
    "sdf.select(\"review_body_length\", \"review_headline_length\").summary().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ab4e74f-df7c-458f-aac6-ebf81ae0f7bf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Filter by removing short reviews\n",
    "sdf = sdf.where(sdf.review_body_length > 10)\n",
    "sdf = sdf.where(sdf.review_headline_length > 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7812c8d1-bb80-45bc-ae7c-22623bb68940",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Re-check the cleaned headline and body\n",
    "\n",
    "print('review_headline: ')\n",
    "sdf.select(\"review_headline\").summary(\"count\", \"min\",\n",
    "\"max\").show()\n",
    "\n",
    "print('review_body: ')\n",
    "sdf.select(\"review_body\").summary(\"count\", \"min\",\n",
    "\"max\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d4fcef4a-3a9b-4b96-ba9d-869c98b8cba7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Convert data types if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8304b1e5-936a-4eb3-a4af-d26aaf5c6de0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Convert 'star_rating' column from string to integer using cast\n",
    "sdf = sdf.withColumn(\"star_rating\", col(\"star_rating\").cast(\"int\"))\n",
    "\n",
    "# Print the schema after the conversion\n",
    "sdf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "016d621a-dd3e-4cf0-9aac-d040a1b3c2d6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Create a few extra columns based on the date\n",
    "**This will be useful for when you are running statistical analysis by year, month, or day**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7455149-ca2a-4c59-9745-47e7c6362e1a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import year, month, date_format, when\n",
    "\n",
    "# Create a few extra columns based on the date\n",
    "sdf = sdf.withColumn(\"review_year\", year(col(\"review_date\")))\n",
    "sdf = sdf.withColumn(\"review_month\", month(col(\"review_date\")))\n",
    "sdf = sdf.withColumn(\"review_yearmonth\", date_format(col(\"review_date\"), \"yyyy-MM\"))   # Like 2023-01   2023-02 etc.\n",
    "sdf = sdf.withColumn(\"review_dayofweek\", date_format(col(\"review_date\"), \"E\"))         # 'Monday' 'Tuesday' etc.\n",
    "sdf = sdf.withColumn(\"review_weekend\", when(sdf.review_dayofweek == 'Saturday',1.0).when(sdf.review_dayofweek == 'Sunday', 1.0).otherwise(0))\n",
    "\n",
    "# Print the schema after the splitting the review_date column\n",
    "sdf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6741f5b8-902e-4de9-aeb5-c9814ce0ca70",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Get some statistics and graphs of your cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "afd88409-9452-4576-9637-2e8794b54be9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get some statistics on each of the columns (Warning: This can take a long time, unless you run it on the sample sdf)\n",
    "sdf.summary().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b35c9f25-3dbd-4e6c-b822-9196c9804611",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Take a sample of the data without replacement\n",
    "sdf_sample = sdf.sample(False, 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "269db916-cd51-424a-ac36-771dd8983831",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Look at statistics for some specific columns\n",
    "# Note: Here we are only taking the statistics from a sample of the date\n",
    "# To take statistics from the full data set, please replace 'sdf_sample' with 'sdf'\n",
    "sdf_sample.select(\"star_rating\", \"helpful_votes\", \"total_votes\").summary(\"count\", \"min\",\n",
    "\"max\", \"mean\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f50eaa65-7bda-43f4-8978-621b76597bdf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Look at the statistics for the Review headline and Review Body\n",
    "sdf_sample.select(\"vine\", \"verified_purchase\", \"review_body\").summary(\"count\", \"min\", \"max\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "997dfe0e-73fe-443b-8b1b-6430aceaab9f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Produce a bar chart that displays Number of Reviews by Review Date\n",
    "import matplotlib.pyplot as plt\n",
    "# Use groupby to get a count by date. Then convert to pandas dataframe\n",
    "df = sdf.groupby(\"review_date\").count().sort(\"review_date\").toPandas()\n",
    "\n",
    "# Matplotlib create a figure\n",
    "fig = plt.figure(facecolor='white')\n",
    "plt.bar(df['review_date'], df['count'])\n",
    "# Set the x-axis label\n",
    "plt.xlabel(\"Review Date\")\n",
    "# Set the y-axis label\n",
    "plt.ylabel(\"Number of Reviews\")\n",
    "# Set the Title of the plot\n",
    "plt.title(\"Number of Reviews by Date\")\n",
    "plt.xticks(rotation=90, ha='right')\n",
    "fig.tight_layout()\n",
    "# Save the figure\n",
    "plt.savefig(\"review_count_by_date_matplotlib.png\")\n",
    "# If running in Jupyter Notebook call plt.show()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3dd11f80-1776-4162-9d72-cc17183931cf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Save the figure\n",
    "from io import BytesIO\n",
    "\n",
    "# Save the figure to a BytesIO object\n",
    "img_data = BytesIO()\n",
    "fig.savefig(img_data, format='png')\n",
    "img_data.seek(0)\n",
    "\n",
    "# Your S3 bucket details\n",
    "# Replace 'bucket-name-xx' with your actual bucket name. Replace 'xx' with your initials.\n",
    "bucket_name = 'bucket-name-xx'\n",
    "s3_path = 'models/'\n",
    "\n",
    "# Upload the figure to S3 using boto3\n",
    "# Replace 'your-access-key' and 'your-secret-key' with your actual AWS access/secret keys\n",
    "s3 = boto3.client('s3', aws_access_key_id='your-access-key', aws_secret_access_key='your-secret-key')\n",
    "s3.upload_fileobj(img_data, bucket_name, s3_path)\n",
    "\n",
    "# Close the plot\n",
    "plt.close()\n",
    "\n",
    "# Print statement\n",
    "print(f\"Figure saved to S3: {bucket_name}/{s3_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b793dfa7-1dda-4986-899e-3cbce18b1c3a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Produce a bar chart that displays Number of Star Ratings by Star Rating\n",
    "import matplotlib.pyplot as plt\n",
    "# Use groupby to get a count by rating. Then convert to pandas dataframe\n",
    "df = sdf.groupby(\"star_rating\").count().sort(\"star_rating\").toPandas()\n",
    "\n",
    "# Matplotlib create a figure\n",
    "fig = plt.figure(facecolor='white')\n",
    "plt.bar(df['star_rating'], df['count'])\n",
    "# Set the x-axis label\n",
    "plt.xlabel(\"Star Rating\")\n",
    "# Set the y-axis label\n",
    "plt.ylabel(\"Number of Star_Ratings\")\n",
    "# Set the Title of the plot\n",
    "plt.title(\"Number of Star Ratings by Star Rating\")\n",
    "plt.xticks(rotation=90, ha='right')\n",
    "fig.tight_layout()\n",
    "# Save the figure\n",
    "plt.savefig(\"Number_of_Star_Rating_by_Star_Rating.png\")\n",
    "# If running in Jupyter Notebook call plt.show()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20ae304e-0f09-4302-810c-5e13bec87140",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Save the figure\n",
    "from io import BytesIO\n",
    "\n",
    "# Save the figure to a BytesIO object\n",
    "img_data = BytesIO()\n",
    "fig.savefig(img_data, format='png')\n",
    "img_data.seek(0)\n",
    "\n",
    "# Your S3 bucket details\n",
    "# Replace 'bucket-name-xx' with your actual bucket name. Replace 'xx' with your initials.\n",
    "bucket_name = 'bucket-name-xx'\n",
    "s3_path = 'models/'\n",
    "\n",
    "# Upload the figure to S3 using boto3\n",
    "# Replace 'your-access-key' and 'your-secret-key' with your actual AWS access/secret keys\n",
    "s3 = boto3.client('s3', aws_access_key_id='your-access-key', aws_secret_access_key='your-secret-key')\n",
    "s3.upload_fileobj(img_data, bucket_name, s3_path)\n",
    "\n",
    "# Close the plot\n",
    "plt.close()\n",
    "\n",
    "# Print statement\n",
    "print(f\"Figure saved to S3: {bucket_name}/{s3_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "912f7936-2b0e-461d-b373-180f6ab0d1a7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Produce a bar chart that displays Number of Reviews by Day of the Week\n",
    "import matplotlib.pyplot as plt\n",
    "# Use groupby to get a count of reviews by day of the week. Then convert to pandas dataframe\n",
    "df = sdf.groupby(\"review_dayofweek\").count().sort(\"review_dayofweek\").toPandas()\n",
    "\n",
    "# Matplotlib create a figure\n",
    "fig = plt.figure(facecolor='white')\n",
    "plt.bar(df['review_dayofweek'], df['count'])\n",
    "# Set the x-axis label\n",
    "plt.xlabel(\"Day of the Week\")\n",
    "# Set the y-axis label\n",
    "plt.ylabel(\"Number of Reviews\")\n",
    "# Set the Title of the plot\n",
    "plt.title(\"Number of Reviews by Day of the Week\")\n",
    "plt.xticks(rotation=90, ha='right')\n",
    "fig.tight_layout()\n",
    "# Save the figure\n",
    "plt.savefig(\"Number_of_Reviews_by_DayofWeek.png\")\n",
    "# If running in Jupyter Notebook call plt.show()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09acda12-bd5b-4302-9d77-d5121363979e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Save the figure\n",
    "from io import BytesIO\n",
    "\n",
    "# Save the figure to a BytesIO object\n",
    "img_data = BytesIO()\n",
    "fig.savefig(img_data, format='png')\n",
    "img_data.seek(0)\n",
    "\n",
    "# Your S3 bucket details\n",
    "# Replace 'bucket-name-xx' with your actual bucket name. Replace 'xx' with your initials.\n",
    "bucket_name = 'bucket-name-xx'\n",
    "s3_path = 'models/'\n",
    "\n",
    "# Upload the figure to S3 using boto3\n",
    "# Replace 'your-access-key' and 'your-secret-key' with your actual AWS access/secret keys\n",
    "s3 = boto3.client('s3', aws_access_key_id='your-access-key', aws_secret_access_key='your-secret-key')\n",
    "s3.upload_fileobj(img_data, bucket_name, s3_path)\n",
    "\n",
    "# Close the plot\n",
    "plt.close()\n",
    "\n",
    "# Print statement\n",
    "print(f\"Figure saved to S3: {bucket_name}/{s3_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a6cc2f4f-20fb-4eb0-8faf-eb60317f6bab",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Save your cleaned Spark DataFrame into your AWS S3 '/raw' folder as a Parquet File\n",
    "**Parquet files tend to read into Spark DataFrames quicker. This will be useful later in our Feature Engineering and Modeling phase**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e2ce467-cca2-4844-9687-7a7f3e95add2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Specify your S3 bucket and path\n",
    "# Replace \"bucket-name-xx\" with your actual bucket name and replace 'xx' with your initials.\n",
    "s3_bucket = \"bucket-name-xx\"\n",
    "s3_path = \"s3a://{}/raw/amazon_reviews_us_Beauty_v1_00_cleaned.parquet\".format(s3_bucket)\n",
    "\n",
    "# Save the DataFrame to S3 in Parquet format\n",
    "# Note: \"overwrite\" will save over any previous files in the s3_path with the same file name.\n",
    "sdf.write.parquet(s3_path, mode=\"overwrite\")\n",
    "\n",
    "# Display a success message\n",
    "print(\"DataFrame saved to S3 successfully!: {}\".format(s3_path))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Final_EDA_Cleaning_Script",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
